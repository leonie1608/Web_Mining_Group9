{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d960a8-cce6-4f65-8621-bedc8ae3302b",
   "metadata": {},
   "source": [
    "# Content-Based Filtering\n",
    "with following Model:\n",
    "- TF-IDF with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3abd67-38f8-46ee-8294-a76812fa44e9",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f5733-8ad7-4396-81a4-b9a6251cadaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install the surprise package\n",
    "!pip install -q -U scikit-surprise\n",
    "from surprise import Dataset, Reader\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.dataset import DatasetAutoFolds\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import KNNBasic\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "import random\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('stopwords')\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af6dfc-fa37-4ce4-b3b2-cdad1699c340",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e78963a5-223b-494f-a118-53ac04792f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.expanduser('../data/data_kindle_preprocessed_smaller.xlsx')\n",
    "data_preprocessed = pd.read_excel(path, index_col=[0], dtype={'publication_year': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85f47aee-d925-4bcc-8fc0-31ee98013321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367733     Kindle Store, Kindle eBooks, Romance  Visit Am...\n",
       "853589     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "1034265    Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "450140     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "558308     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "                                 ...                        \n",
       "864078     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "97789      Kindle Store, Kindle eBooks, Mystery, Thriller...\n",
       "248284     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "619502     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "901171     Kindle Store, Kindle eBooks, Literature & Fict...\n",
       "Name: book_info, Length: 33036, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed['book_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a1493a4b-bbd0-4309-8a79-74624e003fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in case for content analysis\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # cleaning \n",
    "    import re \n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n",
    "    remove_white_space = remove_punctuation.strip()\n",
    "\n",
    "    # Tokenization = Breaking down each sentence into an array\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_text = word_tokenize(remove_white_space)\n",
    "\n",
    "    # Stop Words/filtering = Removing irrelevant words\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    stopwords_removed = [word for word in tokenized_text if word not in stopwords]\n",
    "\n",
    "    # Stemming = Transforming words into their base form\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n",
    "    \n",
    "    return stemmed_text  # Return only the stemmed text\n",
    "\n",
    "\n",
    "# Apply preprocess_text function to book_info\n",
    "data_preprocessed['book_info'] = data_preprocessed['book_info'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0aeca54-007c-4aaa-94b3-b8eb6ad5d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subset of data_preprocessing to create new subset of columns\n",
    "data_contentBased = data_preprocessed[[\"reviewerID\", \"asin\", \"rating\", \"book_info\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d608994-9e08-4f22-bc44-96e71ac54b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonie\\AppData\\Local\\Temp\\ipykernel_16804\\2135891888.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_contentBased['book_info'] = data_contentBased['book_info'].apply(lambda x: ' '.join(x))\n"
     ]
    }
   ],
   "source": [
    "data_contentBased['book_info'] = data_contentBased['book_info'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3afd5371-2875-4b72-b2f1-8a6444ba79a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>book_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367733</th>\n",
       "      <td>A1P44JJW47E6QN</td>\n",
       "      <td>B00DVWRDN6</td>\n",
       "      <td>5</td>\n",
       "      <td>kindl store kindl ebook romanc visit amazon sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853589</th>\n",
       "      <td>A1BOK1EC36Q0WV</td>\n",
       "      <td>B00RE0O2OO</td>\n",
       "      <td>5</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034265</th>\n",
       "      <td>A1ZV5XBKG7RVZU</td>\n",
       "      <td>B013GZOXPS</td>\n",
       "      <td>5</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450140</th>\n",
       "      <td>A1MXW3BGAZW44D</td>\n",
       "      <td>B00G5IG3RK</td>\n",
       "      <td>4</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558308</th>\n",
       "      <td>A1VDXZASTPFIAE</td>\n",
       "      <td>B00IL9ZCBY</td>\n",
       "      <td>4</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864078</th>\n",
       "      <td>A39YNUAPSSOD82</td>\n",
       "      <td>B00RY2IAMM</td>\n",
       "      <td>4</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97789</th>\n",
       "      <td>A1AMO4S4I575LQ</td>\n",
       "      <td>B006LWJ75K</td>\n",
       "      <td>4</td>\n",
       "      <td>kindl store kindl ebook mysteri thriller suspe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248284</th>\n",
       "      <td>A1UVH2I7WHDWLD</td>\n",
       "      <td>B00BBQ2JPQ</td>\n",
       "      <td>5</td>\n",
       "      <td>kindl store kindl ebook literatur fiction kell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619502</th>\n",
       "      <td>A2B9C3FMYW18UN</td>\n",
       "      <td>B00K31DCD8</td>\n",
       "      <td>3</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901171</th>\n",
       "      <td>A10PIB0GJZQHJA</td>\n",
       "      <td>B00TPBIA4S</td>\n",
       "      <td>5</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33036 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             reviewerID        asin  rating  \\\n",
       "367733   A1P44JJW47E6QN  B00DVWRDN6       5   \n",
       "853589   A1BOK1EC36Q0WV  B00RE0O2OO       5   \n",
       "1034265  A1ZV5XBKG7RVZU  B013GZOXPS       5   \n",
       "450140   A1MXW3BGAZW44D  B00G5IG3RK       4   \n",
       "558308   A1VDXZASTPFIAE  B00IL9ZCBY       4   \n",
       "...                 ...         ...     ...   \n",
       "864078   A39YNUAPSSOD82  B00RY2IAMM       4   \n",
       "97789    A1AMO4S4I575LQ  B006LWJ75K       4   \n",
       "248284   A1UVH2I7WHDWLD  B00BBQ2JPQ       5   \n",
       "619502   A2B9C3FMYW18UN  B00K31DCD8       3   \n",
       "901171   A10PIB0GJZQHJA  B00TPBIA4S       5   \n",
       "\n",
       "                                                 book_info  \n",
       "367733   kindl store kindl ebook romanc visit amazon sh...  \n",
       "853589   kindl store kindl ebook literatur fiction visi...  \n",
       "1034265  kindl store kindl ebook literatur fiction visi...  \n",
       "450140   kindl store kindl ebook literatur fiction visi...  \n",
       "558308   kindl store kindl ebook literatur fiction visi...  \n",
       "...                                                    ...  \n",
       "864078   kindl store kindl ebook literatur fiction visi...  \n",
       "97789    kindl store kindl ebook mysteri thriller suspe...  \n",
       "248284   kindl store kindl ebook literatur fiction kell...  \n",
       "619502   kindl store kindl ebook literatur fiction visi...  \n",
       "901171   kindl store kindl ebook literatur fiction visi...  \n",
       "\n",
       "[33036 rows x 4 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_contentBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "602494ee-a0d1-46de-a046-abbe3ccf96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the data into training and testing sets\n",
    "X = data_contentBased['book_info']  # Independent variable - contains category_string, brand, paid_free, print_length_category, publication_year, language\n",
    "y = data_contentBased['asin']  # Dependent variable (product ID)\n",
    "\n",
    "# split training and testdata. random_state 42 is the same as for the split of the collaborative filtering models, to make sure to use the same training/testsplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87b386-6b15-441e-b2e3-343b9db4bb7c",
   "metadata": {},
   "source": [
    "# Content Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2861e-fc69-4942-ae0e-d7d2bcfe72a1",
   "metadata": {},
   "source": [
    "### With TF-IDF Verctorizer and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4fdf2b1b-ad9d-4d03-ac80-b51490b70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_recommender(data_contentBased):\n",
    "    \n",
    "    # Define the number of similar ASINs to retrieve for each ASIN\n",
    "    n_recommendations = 10\n",
    "    \n",
    "    # Feature Extraction\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(data_contentBased['book_info'])\n",
    "    \n",
    "    # Similarity Calculation\n",
    "    similarity_matrix = cosine_similarity(X_tfidf)\n",
    "    \n",
    "    # Recommendation\n",
    "    # Create a dictionary to store similar ASINs for each ASIN\n",
    "    similar_asins_dict = {}\n",
    "    \n",
    "    # Iterate through each ASIN\n",
    "    for i, asin in enumerate(data_contentBased['asin']):\n",
    "        # Find index of current ASIN\n",
    "        asin_index = data_contentBased.index[data_contentBased['asin'] == asin].tolist()[0]\n",
    "        \n",
    "        # Find top similar ASINs based on similarity scores\n",
    "        similar_asins_indices = similarity_matrix[asin_index].argsort()[:-n_recommendations-1:-1]  # Top n similar ASINs\n",
    "        similar_asins = data_contentBased.iloc[similar_asins_indices]['asin'].tolist()\n",
    "        \n",
    "        # Store similar ASINs in the dictionary\n",
    "        similar_asins_dict[asin] = similar_asins\n",
    "    \n",
    "    # Print or use similar_asins_dict as needed\n",
    "    return similar_asins_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168f520-b774-4daf-a125-3d5bf44eb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Prediction(uid='AI38GSFB90M4L', iid='B00QZABE5O', r_ui=2.0, est=2.875, details={'actual_k': 0, 'was_impossible': False}),\n",
    " Prediction(uid='A1OC44Q6V9TT0G', iid='B007XYIGR0', r_ui=5.0, est=4.571428571428571, details={'actual_k': 0, 'was_impossible': False}),\n",
    " Prediction(uid='A2Z7HILLWSTWML', iid='B00DMQ8IJY', r_ui=4.0, est=3.6363636363636362, details={'actual_k': 0, 'was_impossible': False}),\n",
    " Prediction(uid='A2HZ6MGQ6TPNO0', iid='B0070JOQHM', r_ui=5.0, est=4.165094339622642, details={'actual_k': 1, 'was_impossible': False}),\n",
    " Prediction(uid='A2BOKBB1IZA13L', iid='B00LS6MCMY', r_ui=5.0, est=5, details={'actual_k': 1, 'was_impossible': False}),\n",
    " Prediction(uid='A3MTU1QAMYEAX3', iid='B00FFLB6MU', r_ui=5.0, est=3.6845090393477484, details={'actual_k': 1, 'was_impossible': False}),\n",
    " Prediction(uid='A293W2MU6STRGP', iid='B00C7HG9KY', r_ui=4.0, est=4.966911764705882, details={'actual_k': 1, 'was_impossible': False}),\n",
    " Prediction(uid='A3HN8JIDYGZVVH', iid='B017JA1SFO', r_ui=5.0, est=5, details={'actual_k': 0, 'was_impossible': False}),\n",
    " Prediction(uid='A1UPRXXTKHCA5', iid='B00M0DQIEC', r_ui=4.0, est=4.057317359642941, details={'actual_k': 1, 'was_impossible': Fa\n",
    "\n",
    "\n",
    "i have this prediction of user-based recommendation called 'prediction_memory_based '. walk through all predictions. as soon as a prediction has k of 0, then another approach is implemented for these predictions. otherwise, if k >0, prediction can be kept. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i have another prediction called 'prediction_tf_idf'. this consists of a dict, where books are key and the value are similar books as a list.\n",
    "\n",
    "{asin:[asin1, asin2...], ...}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "065ec7e4-b26d-46af-8ce9-181e333da0d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 367733 is out of bounds for axis 0 with size 33036",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prediction_tf_idf \u001b[38;5;241m=\u001b[39m \u001b[43mtf_idf_recommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_contentBased\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[104], line 23\u001b[0m, in \u001b[0;36mtf_idf_recommender\u001b[1;34m(data_contentBased)\u001b[0m\n\u001b[0;32m     20\u001b[0m asin_index \u001b[38;5;241m=\u001b[39m data_contentBased\u001b[38;5;241m.\u001b[39mindex[data_contentBased[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m asin]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Find top similar ASINs based on similarity scores\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m similar_asins_indices \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43masin_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39margsort()[:\u001b[38;5;241m-\u001b[39mn_recommendations\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Top n similar ASINs\u001b[39;00m\n\u001b[0;32m     24\u001b[0m similar_asins \u001b[38;5;241m=\u001b[39m data_contentBased\u001b[38;5;241m.\u001b[39miloc[similar_asins_indices][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Store similar ASINs in the dictionary\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 367733 is out of bounds for axis 0 with size 33036"
     ]
    }
   ],
   "source": [
    "prediction_tf_idf = tf_idf_recommender(data_contentBased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f2011-b0ec-4cb3-8763-8e2cc370b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92efb51-dc8c-45e3-a4ba-76b9b532011e",
   "metadata": {},
   "source": [
    "# Hybrid Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33cde6-1357-4af3-94d0-cfdb9ccd0cc5",
   "metadata": {},
   "source": [
    "### User-Based Collaborative Filtering + Content-Based Filtering\n",
    "Run after these predictions are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba7b3fc9-f4a5-4e69-aad0-2b4521cdd10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MAE, RMSE of algorithm DummyAlgorithm on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     0.6498  0.6231  0.6362  0.6226  0.6094  0.6282  0.0137  \n",
      "RMSE (testset)    1.1467  1.1137  1.1173  1.1089  1.0936  1.1161  0.0173  \n",
      "Fit time          0.00    0.00    0.01    0.01    0.01    0.00    0.00    \n",
      "Test time         0.05    0.06    0.06    0.06    0.05    0.06    0.00    \n",
      "MAE: 0.6052631578947368\n",
      "RMSE: 1.038723913473187\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy algorithm class that does nothing for the DummyAlgorithm class \n",
    "# that initializes the trainset attribute with a dummy value. Here's how you can modify the code:\n",
    "\n",
    "class DummyAlgorithm(AlgoBase):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        self.trainset = trainset  # Initialize trainset attribute with a dummy value\n",
    "        pass\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return 5  # Return a dummy rating of 5 for all predictions\n",
    "\n",
    "\n",
    "def hybrid_recommender_system(prediction, data_contentBased, data):\n",
    "    # Convert prediction to a dictionary with user IDs as keys\n",
    "    user_item_recs = {}\n",
    "    for pred in prediction:\n",
    "        user_id = pred.uid\n",
    "        item_id = pred.iid\n",
    "        if user_id not in user_item_recs:\n",
    "            user_item_recs[user_id] = []\n",
    "        user_item_recs[user_id].append(item_id)\n",
    "\n",
    "    # Initialize dictionary for hybrid recommendations\n",
    "    hybrid_recs = {}\n",
    "\n",
    "    # For each user, generate recommendations\n",
    "    for user_id, items in user_item_recs.items():\n",
    "        # Initialize list to store recommendations for the user\n",
    "        user_recommendations = items.copy()  # Store original recommendations\n",
    "\n",
    "        # For each item recommended to the user\n",
    "        for item in items:\n",
    "            # Get similar item recommendations from TF-IDF\n",
    "            similar_items = data_contentBased.get(item)\n",
    "\n",
    "            # Check if similar_items is not None before iterating\n",
    "            if similar_items is not None:\n",
    "                # Extend user_recommendations with similar items (excluding already recommended items)\n",
    "                user_recommendations.extend([item for item in similar_items if item not in items])\n",
    "\n",
    "        # Count occurrences of each item ID (ASIN) in user_recommendations\n",
    "        item_counts = {}\n",
    "        for item in user_recommendations:\n",
    "            if item in item_counts:\n",
    "                item_counts[item] += 1\n",
    "            else:\n",
    "                item_counts[item] = 1\n",
    "\n",
    "        # Sort items by count (descending order) and prioritize original recommendations if counts are equal\n",
    "        sorted_items = sorted(item_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        # Take the top ten ASINs from sorted_items\n",
    "        top_ten_asins = [item[0] for item in sorted_items[:10]]\n",
    "\n",
    "        # Add user recommendations to the hybrid recommendations\n",
    "        hybrid_recs[user_id] = top_ten_asins\n",
    "\n",
    "    # Cross-validate the hybrid recommender system\n",
    "    dummy_algo = DummyAlgorithm()\n",
    "    cross_val_results = cross_validate(dummy_algo, data, measures=['mae', 'rmse'], cv=5, verbose=True)\n",
    "    avg_mae = np.mean(cross_val_results['test_mae'])\n",
    "    avg_rmse = np.mean(cross_val_results['test_rmse'])\n",
    "\n",
    "    # Calculate MAE and RMSE on the test data\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for user_id, items in hybrid_recs.items():\n",
    "        for item in items:\n",
    "            actual_rating = [rating for (uid, iid, rating, _) in data.raw_ratings if uid == user_id and iid == item]\n",
    "            if actual_rating:  # Check if actual rating exists\n",
    "                actual_ratings.append(actual_rating[0])\n",
    "                predicted_ratings.append(5)  # Assuming all predicted ratings are 5 (can be replaced with actual predictions)\n",
    "\n",
    "    mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "    # Print MAE and RMSE\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "    return hybrid_recs, avg_mae, avg_rmse, mae, rmse\n",
    "\n",
    "# Call the hybrid recommender system function\n",
    "prediction_hybrid, avg_mae, avg_rmse, mae, rmse = hybrid_recommender_system(prediction_user_based_KNNWithMeans, pred_content_based_recommender_system, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ab695-a2d8-45b9-8e02-0a8b13914f60",
   "metadata": {},
   "source": [
    "# Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e41acd77-bbe4-481c-b095-5ea2e1c7a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634843b-a567-4833-9b4b-55aa3fe5c47a",
   "metadata": {},
   "source": [
    "### Create artificially a colds-start Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f685cd8d-daa2-468c-8cc7-e4fc090aa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_ground_truth = Dataset.load_from_df(data_preprocessed[['reviewerID', 'asin', 'rating']], reader)\n",
    "\n",
    "# Extract 10% of users and their entries\n",
    "unique_users = data_preprocessed['reviewerID'].unique()\n",
    "num_test_users = int(0.1 * len(unique_users))\n",
    "test_users = np.random.choice(unique_users, num_test_users, replace=False)\n",
    "test_entries = data_preprocessed[data_preprocessed['reviewerID'].isin(test_users)]\n",
    "\n",
    "# Remove test users and their entries from the dataset\n",
    "data_remaining = data_preprocessed[~data_preprocessed['reviewerID'].isin(test_users)]\n",
    "#print(data_remaining['reviewerID'])\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "data = Dataset.load_from_df(data_remaining[['reviewerID', 'asin', 'rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets using Surprise's train_test_split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test_entries to the same format as testset\n",
    "test_entries_tuples = [tuple(x) for x in test_entries[['reviewerID', 'asin', 'rating']].values]\n",
    "\n",
    "\n",
    "# Load the additional rows into the Surprise Dataset object\n",
    "testset+= test_entries_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac0c9026-892a-4077-b914-3d52e16df201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique reviewer IDs in train set: 812\n",
      "Number of unique reviewer IDs in test set: 902\n",
      "Users in test set but not in train set: 90\n",
      "Top ten users in test set but not in train set:\n",
      "A29A1CPI3AOYSD\n",
      "A2D9S6V1AFXOAO\n",
      "ACATGMB8JTGU9\n",
      "A73IK4VT0XZS6\n",
      "A1DC00T32U26HC\n",
      "AYXKCV0BGC0PU\n",
      "A2W3RQOSC87P4A\n",
      "A2UNMDJYXPEQZ3\n",
      "A2MIR34MGFG3EU\n",
      "A1XNUZ8KUU5UYX\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty sets to collect unique reviewer IDs\n",
    "unique_users_train = set()\n",
    "unique_users_test = set()\n",
    "\n",
    "# Iterate through the trainset ratings and collect unique reviewer IDs\n",
    "for (user_id, _, _) in trainset.all_ratings():\n",
    "    unique_users_train.add(user_id)\n",
    "\n",
    "# Iterate through the testset ratings and collect unique reviewer IDs\n",
    "for (user_id, _, _) in testset:\n",
    "    unique_users_test.add(user_id)\n",
    "\n",
    "# Get the number of unique reviewer IDs in train and test sets\n",
    "num_unique_users_train = len(unique_users_train)\n",
    "num_unique_users_test = len(unique_users_test)\n",
    "test_only_users_count = len(unique_users_test) - len(unique_users_train)\n",
    "test_only_users = unique_users_test - unique_users_train\n",
    "\n",
    "print(\"Number of unique reviewer IDs in train set:\", num_unique_users_train)\n",
    "print(\"Number of unique reviewer IDs in test set:\", num_unique_users_test)\n",
    "print(\"Users in test set but not in train set:\", test_only_users_count)\n",
    "print(\"Top ten users in test set but not in train set:\")\n",
    "for i, user_id in enumerate(test_only_users):\n",
    "    if i < 10:\n",
    "        print(user_id)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63344c6-58a4-466d-bd09-0cf5f28fcc8c",
   "metadata": {},
   "source": [
    "Here we can see, that trainset contains user ids, that are not in the trainset. Therefore a cold-start problem will occur for memory-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd8d5c42-d8e0-4607-869f-3f76c1a954fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_KNNWithMeans_recommender_system(trainset, testset, data):\n",
    "    \n",
    "    #Choose best algorithm based on grid search\n",
    "    algo = KNNWithMeans(k=1, sim_options={'name': 'pearson', 'user_based': True})\n",
    "    \n",
    "    # Train the best model with the new parameters and evaluate the trained model on the test set\n",
    "    test_pred = algo.fit(trainset).test(testset)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    print(\"User-based Model with KNNWithMeans: Test Set\")\n",
    "    accuracy.rmse(test_pred, verbose=True)\n",
    "    accuracy.mae(test_pred, verbose=True)\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a60c0b0f-47e6-40c6-b18e-e85d9d47e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User-based Model with KNNWithMeans: Test Set\n",
      "RMSE: 0.8764\n",
      "MAE:  0.6396\n"
     ]
    }
   ],
   "source": [
    "prediction_memory_based = user_based_KNNWithMeans_recommender_system(trainset,testset, data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2ca2876-6e68-4900-a105-7b2e8791a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user ID to filter\n",
    "target_user_id = 'A2XNID09ZCM4CP'\n",
    "\n",
    "# Filter predictions for the target user ID\n",
    "user_predictions = [pred for pred in prediction_memory_based if pred.uid == target_user_id]\n",
    "\n",
    "# Print the filtered predictions\n",
    "for pred in user_predictions:\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a4a31-cd10-4a0b-bd34-05c0ee4949ea",
   "metadata": {},
   "source": [
    "In this output we can see: Cold-start problem leads to poor prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e1eef-31f6-4c3e-8565-fd5d6e58e412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
