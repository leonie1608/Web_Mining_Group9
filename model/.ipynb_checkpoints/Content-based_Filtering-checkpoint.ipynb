{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d960a8-cce6-4f65-8621-bedc8ae3302b",
   "metadata": {},
   "source": [
    "# Content-Based Filtering\n",
    "with following Model:\n",
    "- TF-IDF with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3abd67-38f8-46ee-8294-a76812fa44e9",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f5733-8ad7-4396-81a4-b9a6251cadaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install the surprise package\n",
    "!pip install -q -U scikit-surprise\n",
    "from surprise import Dataset, Reader\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.dataset import DatasetAutoFolds\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import KNNBasic\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "import random\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('stopwords')\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af6dfc-fa37-4ce4-b3b2-cdad1699c340",
   "metadata": {},
   "source": [
    "### Data Preparation and Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78963a5-223b-494f-a118-53ac04792f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.expanduser('../data/data_kindle_preprocessed_smaller.xlsx')\n",
    "data_preprocessed = pd.read_excel(path, index_col=[0], dtype={'publication_year': str, 'book_info': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bcbad38-db87-4365-b01f-a309aae1a4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>language</th>\n",
       "      <th>print_length_category</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>category_string</th>\n",
       "      <th>paid_free</th>\n",
       "      <th>book_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527748</th>\n",
       "      <td>4</td>\n",
       "      <td>A1TVJ9WDRP0UZS</td>\n",
       "      <td>B00HY2KN04</td>\n",
       "      <td>Just Jelly Beans and Jealousy (The Reed Brothe...</td>\n",
       "      <td>Visit Amazon's Tammy Falkner Page</td>\n",
       "      <td>English</td>\n",
       "      <td>small</td>\n",
       "      <td>2014</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fiction</td>\n",
       "      <td>Free</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12432</th>\n",
       "      <td>5</td>\n",
       "      <td>A3IQ0P3M39IY8U</td>\n",
       "      <td>B004BLJ9IS</td>\n",
       "      <td>The You I&amp;ve Always Dreamed Of (Finding Famil...</td>\n",
       "      <td>Visit Amazon's Alison Kent Page</td>\n",
       "      <td>English</td>\n",
       "      <td>small</td>\n",
       "      <td>2010</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fiction</td>\n",
       "      <td>Paid</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438646</th>\n",
       "      <td>5</td>\n",
       "      <td>AIQWMQ4JWKZ3T</td>\n",
       "      <td>B00FQAQQ9I</td>\n",
       "      <td>Wyoming Wildflowers: The Beginning: A Prequel ...</td>\n",
       "      <td>Visit Amazon's Patricia McLinn Page</td>\n",
       "      <td>English</td>\n",
       "      <td>small</td>\n",
       "      <td>2014</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fiction</td>\n",
       "      <td>Free</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620082</th>\n",
       "      <td>4</td>\n",
       "      <td>A3CMIEYL0TJLC2</td>\n",
       "      <td>B00K31DCD8</td>\n",
       "      <td>Finding My Prince Charming (The Prince Charmin...</td>\n",
       "      <td>Visit Amazon's J. S. Cooper Page</td>\n",
       "      <td>English</td>\n",
       "      <td>small</td>\n",
       "      <td>2014</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fiction</td>\n",
       "      <td>Paid</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Literature &amp; Fict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590077</th>\n",
       "      <td>5</td>\n",
       "      <td>A1L9WQBSQ5SEFE</td>\n",
       "      <td>B00JD5BO8K</td>\n",
       "      <td>Cowboy Wedding Mix-up - Kindle edition</td>\n",
       "      <td>Visit Amazon's JT Schultz Page</td>\n",
       "      <td>English</td>\n",
       "      <td>small</td>\n",
       "      <td>2014</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Romance</td>\n",
       "      <td>Paid</td>\n",
       "      <td>Kindle Store, Kindle eBooks, Romance  Visit Am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rating      reviewerID        asin  \\\n",
       "527748       4  A1TVJ9WDRP0UZS  B00HY2KN04   \n",
       "12432        5  A3IQ0P3M39IY8U  B004BLJ9IS   \n",
       "438646       5   AIQWMQ4JWKZ3T  B00FQAQQ9I   \n",
       "620082       4  A3CMIEYL0TJLC2  B00K31DCD8   \n",
       "590077       5  A1L9WQBSQ5SEFE  B00JD5BO8K   \n",
       "\n",
       "                                                    title  \\\n",
       "527748  Just Jelly Beans and Jealousy (The Reed Brothe...   \n",
       "12432    The You I&ve Always Dreamed Of (Finding Famil...   \n",
       "438646  Wyoming Wildflowers: The Beginning: A Prequel ...   \n",
       "620082  Finding My Prince Charming (The Prince Charmin...   \n",
       "590077             Cowboy Wedding Mix-up - Kindle edition   \n",
       "\n",
       "                                      brand language print_length_category  \\\n",
       "527748    Visit Amazon's Tammy Falkner Page  English                 small   \n",
       "12432       Visit Amazon's Alison Kent Page  English                 small   \n",
       "438646  Visit Amazon's Patricia McLinn Page  English                 small   \n",
       "620082     Visit Amazon's J. S. Cooper Page  English                 small   \n",
       "590077       Visit Amazon's JT Schultz Page  English                 small   \n",
       "\n",
       "       publication_year                                    category_string  \\\n",
       "527748             2014  Kindle Store, Kindle eBooks, Literature & Fiction   \n",
       "12432              2010  Kindle Store, Kindle eBooks, Literature & Fiction   \n",
       "438646             2014  Kindle Store, Kindle eBooks, Literature & Fiction   \n",
       "620082             2014  Kindle Store, Kindle eBooks, Literature & Fiction   \n",
       "590077             2014               Kindle Store, Kindle eBooks, Romance   \n",
       "\n",
       "       paid_free                                          book_info  \n",
       "527748      Free  Kindle Store, Kindle eBooks, Literature & Fict...  \n",
       "12432       Paid  Kindle Store, Kindle eBooks, Literature & Fict...  \n",
       "438646      Free  Kindle Store, Kindle eBooks, Literature & Fict...  \n",
       "620082      Paid  Kindle Store, Kindle eBooks, Literature & Fict...  \n",
       "590077      Paid  Kindle Store, Kindle eBooks, Romance  Visit Am...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1493a4b-bbd0-4309-8a79-74624e003fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527748     [kindl, store, kindl, ebook, literatur, fictio...\n",
      "12432      [kindl, store, kindl, ebook, literatur, fictio...\n",
      "438646     [kindl, store, kindl, ebook, literatur, fictio...\n",
      "620082     [kindl, store, kindl, ebook, literatur, fictio...\n",
      "590077     [kindl, store, kindl, ebook, romanc, visit, am...\n",
      "                                 ...                        \n",
      "215730     [kindl, store, kindl, ebook, literatur, fictio...\n",
      "1133251    [kindl, store, kindl, ebook, literatur, fictio...\n",
      "1092850    [kindl, store, kindl, ebook, romanc, visit, am...\n",
      "283061     [kindl, store, kindl, ebook, romanc, visit, am...\n",
      "1171136    [kindl, store, kindl, ebook, literatur, fictio...\n",
      "Name: book_info, Length: 11639, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# used in case for content analysis\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # cleaning \n",
    "    import re \n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n",
    "    remove_white_space = remove_punctuation.strip()\n",
    "\n",
    "    # Tokenization = Breaking down each sentence into an array\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokenized_text = word_tokenize(remove_white_space)\n",
    "\n",
    "    # Stop Words/filtering = Removing irrelevant words\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    stopwords_removed = [word for word in tokenized_text if word not in stopwords]\n",
    "\n",
    "    # Stemming = Transforming words into their base form\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n",
    "    \n",
    "    return stemmed_text  # Return only the stemmed text\n",
    "\n",
    "\n",
    "# Ensure the 'book_info' column is treated as strings\n",
    "data_preprocessed['book_info'] = data_preprocessed['book_info'].astype(str)\n",
    "\n",
    "# Apply preprocess_text function to book_info\n",
    "data_preprocessed['book_info'] = data_preprocessed['book_info'].apply(preprocess_text)\n",
    "\n",
    "print(data_preprocessed['book_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0aeca54-007c-4aaa-94b3-b8eb6ad5d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subset of data_preprocessing to create new subset of columns\n",
    "data_contentBased = data_preprocessed[[\"asin\", \"book_info\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d608994-9e08-4f22-bc44-96e71ac54b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonie\\AppData\\Local\\Temp\\ipykernel_12356\\2135891888.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_contentBased['book_info'] = data_contentBased['book_info'].apply(lambda x: ' '.join(x))\n"
     ]
    }
   ],
   "source": [
    "data_contentBased['book_info'] = data_contentBased['book_info'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afd5371-2875-4b72-b2f1-8a6444ba79a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>book_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527748</th>\n",
       "      <td>B00HY2KN04</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12432</th>\n",
       "      <td>B004BLJ9IS</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438646</th>\n",
       "      <td>B00FQAQQ9I</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620082</th>\n",
       "      <td>B00K31DCD8</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590077</th>\n",
       "      <td>B00JD5BO8K</td>\n",
       "      <td>kindl store kindl ebook romanc visit amazon jt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215730</th>\n",
       "      <td>B00AMOOE0Q</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133251</th>\n",
       "      <td>B0196TRK8K</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092850</th>\n",
       "      <td>B0173QJGLW</td>\n",
       "      <td>kindl store kindl ebook romanc visit amazon ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283061</th>\n",
       "      <td>B00C16YL10</td>\n",
       "      <td>kindl store kindl ebook romanc visit amazon wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171136</th>\n",
       "      <td>B01BA8ZTBA</td>\n",
       "      <td>kindl store kindl ebook literatur fiction visi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11639 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               asin                                          book_info\n",
       "527748   B00HY2KN04  kindl store kindl ebook literatur fiction visi...\n",
       "12432    B004BLJ9IS  kindl store kindl ebook literatur fiction visi...\n",
       "438646   B00FQAQQ9I  kindl store kindl ebook literatur fiction visi...\n",
       "620082   B00K31DCD8  kindl store kindl ebook literatur fiction visi...\n",
       "590077   B00JD5BO8K  kindl store kindl ebook romanc visit amazon jt...\n",
       "...             ...                                                ...\n",
       "215730   B00AMOOE0Q  kindl store kindl ebook literatur fiction visi...\n",
       "1133251  B0196TRK8K  kindl store kindl ebook literatur fiction visi...\n",
       "1092850  B0173QJGLW  kindl store kindl ebook romanc visit amazon ta...\n",
       "283061   B00C16YL10  kindl store kindl ebook romanc visit amazon wi...\n",
       "1171136  B01BA8ZTBA  kindl store kindl ebook literatur fiction visi...\n",
       "\n",
       "[11639 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_contentBased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87b386-6b15-441e-b2e3-343b9db4bb7c",
   "metadata": {},
   "source": [
    "# Content Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2861e-fc69-4942-ae0e-d7d2bcfe72a1",
   "metadata": {},
   "source": [
    "### With TF-IDF Verctorizer and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2593aa4-0d58-438b-ae57-fd98573abf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top 10 similar products for each product in the dataframe\n",
    "def tf_idf_recommender(data_preprocessed):\n",
    "    n_recommendations=10\n",
    "    \n",
    "    # Compute TF-IDF matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data_preprocessed['book_info'])\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Create a DataFrame for cosine similarity matrix for better readability\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=data_preprocessed.index, columns=data_preprocessed.index)\n",
    "\n",
    "    # Recommendation\n",
    "    # Create a dictionary to store similar ASINs for each ASIN\n",
    "    similar_asins_dict = {}\n",
    "    \n",
    "    # Iterate through each ASIN\n",
    "    for i, asin in enumerate(data_preprocessed['asin']):\n",
    "        # Find index of current ASIN\n",
    "        asin_index = data_preprocessed.index[i]\n",
    "        \n",
    "        # Find top similar ASINs based on similarity scores\n",
    "        similar_asins_indices = cosine_sim_df[asin_index].argsort()[:-n_recommendations-1:-1]  # Top n similar ASINs\n",
    "        similar_asins = data_preprocessed.iloc[similar_asins_indices]['asin'].tolist()\n",
    "        \n",
    "        # Store similar ASINs in the dictionary\n",
    "        similar_asins_dict[asin] = similar_asins\n",
    "\n",
    "    # Create a new dictionary for filtered recommendations\n",
    "    filtered_recommendations = {}\n",
    "    \n",
    "    # Iterate over the original recommendations\n",
    "    for key, value in similar_asins_dict.items():\n",
    "        # Filter out values that are similar to the key\n",
    "        filtered_values = [v for v in value if v != key]\n",
    "        # Add to the filtered recommendations if there are any remaining values\n",
    "        if filtered_values:\n",
    "            filtered_recommendations[key] = filtered_values\n",
    "\n",
    "    # Iterate over the dictionary and remove duplicate values\n",
    "    for key, value in filtered_recommendations.items():\n",
    "        filtered_recommendations[key] = list(set(value))\n",
    "    \n",
    "    # Print or use similar_asins_dict as needed\n",
    "    return filtered_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9d4944-5256-4e95-b5d2-740172d97092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_recommendations = tf_idf_recommender(data_contentBased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a728069c-d762-497c-8f6e-b92163770d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B00HE4Z51Y',\n",
       " 'B00BL6CDCA',\n",
       " 'B0090W7ZB6',\n",
       " 'B00C2L7N4G',\n",
       " 'B00P32ZUAK',\n",
       " 'B0069FJE4S',\n",
       " 'B005LVV6DI']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_recommendations.get('B00CDZU7SU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a3040d8-3cc8-4a75-9e56-9859542ce966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map ASINs to titles\n",
    "asin_to_title = dict(zip(data_preprocessed['asin'], data_preprocessed['title']))\n",
    "\n",
    "recommendations_with_titles = {}\n",
    "\n",
    "# Iterate over the dictionary with ASIN values and replace both keys and values with titles\n",
    "for key, value in tf_idf_recommendations.items():\n",
    "    key_title = asin_to_title[key]  # Get the title corresponding to the key (ASIN)\n",
    "    value_titles = [asin_to_title[asin] for asin in value]  # Get the titles corresponding to the values (ASINs)\n",
    "    recommendations_with_titles[key_title] = value_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88073bf-d935-4ee8-b3b4-0d2649b00190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Recommendations based on the book 'Ward of the Vampire (Ward of the Vampire Serial Book 1) - Kindle edition':\n",
      "\n",
      "A Twitch of Tail (The Wiccan-Were-Bear Series Book 6) - Kindle edition\n",
      "Loving Lachlyn (Ashland Pride Two) - Kindle edition\n",
      "Seducing Samantha (Ashland Pride One) - Kindle edition\n",
      "Every Dawn Forever (Hyena Heat Two) - Kindle edition\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Recommendations based on the book 'Ward of the Vampire (Ward of the Vampire Serial Book 1) - Kindle edition':\\n\")\n",
    "\n",
    "for i in recommendations_with_titles.get('Ward of the Vampire (Ward of the Vampire Serial Book 1) - Kindle edition'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ab695-a2d8-45b9-8e02-0a8b13914f60",
   "metadata": {},
   "source": [
    "# Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41acd77-bbe4-481c-b095-5ea2e1c7a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634843b-a567-4833-9b4b-55aa3fe5c47a",
   "metadata": {},
   "source": [
    "### Create artificially a colds-start Problem\n",
    "The cold start problem in recommender systems refers to the challenge of providing accurate recommendations for new users or items with limited interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f685cd8d-daa2-468c-8cc7-e4fc090aa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_ground_truth = Dataset.load_from_df(data_preprocessed[['reviewerID', 'asin', 'rating']], reader)\n",
    "\n",
    "# Extract 10% of users and their entries\n",
    "unique_users = data_preprocessed['reviewerID'].unique()\n",
    "num_test_users = int(0.1 * len(unique_users))\n",
    "test_users = np.random.choice(unique_users, num_test_users, replace=False)\n",
    "test_entries = data_preprocessed[data_preprocessed['reviewerID'].isin(test_users)]\n",
    "\n",
    "# Remove test users and their entries from the dataset\n",
    "data_remaining = data_preprocessed[~data_preprocessed['reviewerID'].isin(test_users)]\n",
    "#print(data_remaining['reviewerID'])\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "data = Dataset.load_from_df(data_remaining[['reviewerID', 'asin', 'rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets using Surprise's train_test_split\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test_entries to the same format as testset\n",
    "test_entries_tuples = [tuple(x) for x in test_entries[['reviewerID', 'asin', 'rating']].values]\n",
    "\n",
    "\n",
    "# Load the additional rows into the Surprise Dataset object\n",
    "testset+= test_entries_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac0c9026-892a-4077-b914-3d52e16df201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique reviewer IDs in train set: 314\n",
      "Number of unique reviewer IDs in test set: 348\n",
      "Users in test set but not in train set: 34\n",
      "Top ten users in test set but not in train set:\n",
      "A1VF3B672MSQ4C\n",
      "A2W3RQOSC87P4A\n",
      "AR6WIPHVS5G3I\n",
      "A2C7W167DVNUKH\n",
      "A39N24TTLP6I48\n",
      "A376U5S8TKOE69\n",
      "A2JPS7EMNFKXOV\n",
      "AIQWMQ4JWKZ3T\n",
      "A3O7EX5CU264Y1\n",
      "A3QVW8NFY4C4I1\n",
      "A13LERNQ8R7267\n",
      "A3A7FF87LEVCQ1\n",
      "A36PA4XPATJSKX\n",
      "A13JQT6036JF\n",
      "A1SBQ0F7FSLWB0\n",
      "A1JLU5H1CCENWX\n",
      "A2UNMDJYXPEQZ3\n",
      "A1I96OYAUJ3HQE\n",
      "A916DXE9W36GF\n",
      "AV4HVQ5WUQ1Z1\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty sets to collect unique reviewer IDs\n",
    "unique_users_train = set()\n",
    "unique_users_test = set()\n",
    "\n",
    "# Iterate through the trainset ratings and collect unique reviewer IDs\n",
    "for (user_id, _, _) in trainset.all_ratings():\n",
    "    unique_users_train.add(user_id)\n",
    "\n",
    "# Iterate through the testset ratings and collect unique reviewer IDs\n",
    "for (user_id, _, _) in testset:\n",
    "    unique_users_test.add(user_id)\n",
    "\n",
    "# Get the number of unique reviewer IDs in train and test sets\n",
    "num_unique_users_train = len(unique_users_train)\n",
    "num_unique_users_test = len(unique_users_test)\n",
    "test_only_users_count = len(unique_users_test) - len(unique_users_train)\n",
    "test_only_users = unique_users_test - unique_users_train\n",
    "\n",
    "print(\"Number of unique reviewer IDs in train set:\", num_unique_users_train)\n",
    "print(\"Number of unique reviewer IDs in test set:\", num_unique_users_test)\n",
    "print(\"Users in test set but not in train set:\", test_only_users_count)\n",
    "print(\"Top ten users in test set but not in train set:\")\n",
    "for i, user_id in enumerate(test_only_users):\n",
    "    if i < 20:\n",
    "        print(user_id)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63344c6-58a4-466d-bd09-0cf5f28fcc8c",
   "metadata": {},
   "source": [
    "Here we can see, that trainset contains user ids, that are not in the trainset. Therefore a cold-start problem will occur for memory-based approaches\n",
    "\n",
    "## Hybrid Model\n",
    "This model consists of a Collaborative Filtering Model of choice, in this case, we used the User-Based Filtering with KNNWithMeans. The other recommendation are the top most popular books based on rating and number of reviews. The hybrid apporach aims to solve cold-start problem, meaning that when there is no data known about a user, the most popular books are going to be recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8d5c42-d8e0-4607-869f-3f76c1a954fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_KNNWithMeans_recommender_system(trainset, testset, data):\n",
    "    \n",
    "    #Choose best algorithm based on grid search\n",
    "    algo = KNNWithMeans(k=1, sim_options={'name': 'pearson', 'user_based': False})\n",
    "    \n",
    "    # Train the best model with the new parameters and evaluate the trained model on the test set\n",
    "    test_pred = algo.fit(trainset).test(testset)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    print(\"User-based Model with KNNWithMeans: Test Set\")\n",
    "    accuracy.rmse(test_pred, verbose=True)\n",
    "    accuracy.mae(test_pred, verbose=True)\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60c0b0f-47e6-40c6-b18e-e85d9d47e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User-based Model with KNNWithMeans: Test Set\n",
      "RMSE: 0.9608\n",
      "MAE:  0.7442\n"
     ]
    }
   ],
   "source": [
    "prediction_memory_based = user_based_KNNWithMeans_recommender_system(trainset,testset, data_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2ca2876-6e68-4900-a105-7b2e8791a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: A3QVW8NFY4C4I1 item: B00K6S71RG r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00BBQ2JPQ r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00BPB9L3U r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00DE50LTS r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00KBH7JDI r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00IMNWM3K r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00K40M50E r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B004EYT0UO r_ui = 4.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00A9HW1CY r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B006H9WBWI r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B006HT0TJ0 r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00J3GS5O0 r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00CW44YIQ r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00HZ5ZDX2 r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00G984VPW r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B006JDEK0I r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00AA3P7M8 r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00IFEMVHI r_ui = 4.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00AQR4R9M r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00JNHU0T2 r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B00A6KQUIK r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n",
      "user: A3QVW8NFY4C4I1 item: B005S1XTEA r_ui = 5.00   est = 4.27   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}\n"
     ]
    }
   ],
   "source": [
    "# Define the user ID to filter\n",
    "target_user_id = 'A3QVW8NFY4C4I1'\n",
    "\n",
    "# Filter predictions for the target user ID\n",
    "user_predictions = [pred for pred in prediction_memory_based if pred.uid == target_user_id]\n",
    "\n",
    "# Print the filtered predictions\n",
    "for pred in user_predictions:\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a4a31-cd10-4a0b-bd34-05c0ee4949ea",
   "metadata": {},
   "source": [
    "In this output we can see: Cold-start problem leads to poor prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c6e1eef-31f6-4c3e-8565-fd5d6e58e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_popular_books(data_preprocessed, n=5):\n",
    "    # Group by 'asin' and calculate the average rating and count of ratings for each book\n",
    "    popular_books = data_preprocessed.groupby('asin').agg({'rating': ['mean', 'count']})\n",
    "    popular_books.columns = ['avg_rating', 'rating_count']\n",
    "    \n",
    "    # Sort the books based on rating count and average rating\n",
    "    popular_books = popular_books.sort_values(by=['rating_count', 'avg_rating'], ascending=False)\n",
    "    \n",
    "    # Get the top n popular books\n",
    "    top_books = popular_books.head(n)\n",
    "    \n",
    "    # Return the top books as a list\n",
    "    return top_books.index.tolist()\n",
    "\n",
    "# Example usage:\n",
    "top_popular_books = top_popular_books(data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f019607a-1f0e-4d64-a2cf-e503c8fed93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B00J0CAR16', 'B00EC197UC', 'B00D9CMLNA', 'B00M0DQIEC', 'B00EAHZR5W']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_popular_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88226517-7469-49e1-9a03-cb1e8fe61d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Prediction\n",
    "from collections import defaultdict\n",
    "\n",
    "def hybrid_recommendation(prediction_memory_based, top_popular_books):\n",
    "    # Dictionary to store the count of deleted predictions for each user\n",
    "    deleted_predictions_count = defaultdict(int)\n",
    "    \n",
    "    # New list to store valid predictions\n",
    "    valid_predictions = []\n",
    "    \n",
    "    # Loop through all predictions\n",
    "    for prediction in prediction_memory_based:\n",
    "        # Check if the prediction was impossible\n",
    "        if prediction.details['was_impossible']:\n",
    "            # Increment the count of deleted predictions for this user\n",
    "            deleted_predictions_count[prediction.uid] += 1\n",
    "        else:\n",
    "            # Add valid predictions to the new list\n",
    "            valid_predictions.append(prediction)\n",
    "    \n",
    "    # Loop through deleted predictions count for each user\n",
    "    for user_id, count in deleted_predictions_count.items():\n",
    "        # Add new predictions based on top popular books for this user\n",
    "        for i in range(min(count, 5)):\n",
    "            if i < len(top_popular_books):\n",
    "                new_prediction = Prediction(uid=user_id, iid=top_popular_books[i], r_ui=None, est=5, details={'actual_k': 0, 'was_impossible': False})\n",
    "                valid_predictions.append(new_prediction)\n",
    "    \n",
    "    return valid_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "009de2d8-aa75-4404-84bd-bc3611bcddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = hybrid_recommendation(prediction_memory_based, top_popular_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56b4b3b0-ca1d-4e93-b86a-8d51ef6d8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: A3QVW8NFY4C4I1 item: B00J0CAR16 r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00EC197UC r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00D9CMLNA r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00M0DQIEC r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00EAHZR5W r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00J0CAR16 r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00EC197UC r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00D9CMLNA r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00M0DQIEC r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n",
      "user: A3QVW8NFY4C4I1 item: B00EAHZR5W r_ui = None   est = 5.00   {'actual_k': 0, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# Define the user ID to filter\n",
    "#target_user_id = 'A1JLU5H1CCENWX' using the same target id\n",
    "\n",
    "# Filter predictions for the target user ID\n",
    "user_predictions_hybrid = [pred for pred in predictions if pred.uid == target_user_id]\n",
    "\n",
    "# Print the filtered predictions\n",
    "for pred in user_predictions_hybrid:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ac5f21c-3bc5-4151-ab57-6a1ce5bc0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def serialize_predictions(predictions, filename):\n",
    "    \"\"\"\n",
    "    Serialize a list of Prediction objects to JSON and save it to a file.\n",
    "\n",
    "    Args:\n",
    "    - predictions (list): List of Prediction objects.\n",
    "    - filename (str): Name of the file to save the serialized predictions.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    serialized_predictions = []\n",
    "    for pred in predictions:\n",
    "        serialized_prediction = {\n",
    "            \"uid\": pred.uid,\n",
    "            \"iid\": pred.iid,\n",
    "            \"r_ui\": pred.r_ui,\n",
    "            \"est\": pred.est,\n",
    "            \"details\": pred.details\n",
    "        }\n",
    "        serialized_predictions.append(serialized_prediction)\n",
    "\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(serialized_predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1f9e874-f1da-489f-8ca9-ac7d51967e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_predictions(predictions, '../data/hybrid_recommender_system.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
